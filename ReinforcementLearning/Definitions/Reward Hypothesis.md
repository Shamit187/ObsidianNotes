> That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).

Rewards aren't the place where we should focus on prior knowledge!

For example, Do not put reward in achieving subgoal in a chess game. Reward should be given only to winning.

The reward signal is your way of communicating to the agent what you want achieved, not how you want it achieved

We can formalize reward generated in [[Agent-Environment Interaction]] in two ways...
[[Episodic Task]] & [[Continuing Task]] 